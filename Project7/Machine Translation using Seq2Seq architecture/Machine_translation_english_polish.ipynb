{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 :- Importing necessary libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import os\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional,Dense, CuDNNGRU, CuDNNLSTM, RepeatVector, TimeDistributed, BatchNormalization, Embedding\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 :- Importing the dataset\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    We have the dataset in form of a txt file. We will read it in a format that will make seperation of each individual english to polish translations easy <br><br>\n",
    "<b>The dataset is loaded from the site :- https://www.manythings.org/anki/ </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hi.', 'Cześć.'], ['Run!', 'Uciekaj!'], ['Run.', 'Biegnij.'], ['Run.', 'Uciekaj.'], ['Who?', 'Kto?'], ['Wow!', 'O, dziamdzia zaprzała jej szadź!'], ['Wow!', 'Łał!'], ['Help!', 'Pomocy!'], ['Jump.', 'Skok.']]\n"
     ]
    }
   ],
   "source": [
    "input_file_path     = r'D:\\kaggle_trials\\polish_to_english'+'\\\\pol.txt'\n",
    "file                = open(input_file_path,mode = 'rt',encoding='utf-8')\n",
    "text_to_read        = file.read()\n",
    "file.close()\n",
    "lines               = text_to_read.strip().split('\\n')\n",
    "text_translate      = [line.split('\\t') for line in  lines]\n",
    "print(text_translate[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 :- Preprocessing of the text data\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Step 1:</b> Splitting the data and removal of all special characters. <br>\n",
    "<b>Step 2:</b> Removal of digits from the text.<br>\n",
    "<b>Step 3:</b> Lower-case all the text<br><br>\n",
    "Also the Polish language contains some special characters which are not present in the English language. The polish language contains 32 alphabets. They exclude V and X letters from english and have 9 diacritics. While preprocessing we have removed the diacritics from the Polish language. However, a commented segment can be uncommented to preserve the diacritics    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations removed and all the sentences converted to smaller case.\n"
     ]
    }
   ],
   "source": [
    "clean_text_translated     = []\n",
    "for i in range(len(text_translate)):\n",
    "    text_pair             = text_translate[i]\n",
    "    eng_text              = text_pair[0]\n",
    "    pol_text              = text_pair[1]\n",
    "    \n",
    "    eng_text_split        = eng_text.split()\n",
    "    pol_text_split        = pol_text.split()\n",
    "    \n",
    "    eng_text_punc_removed = ' '.join([re.sub('[^A-Za-z]+', '', eng_text_split[i]) for i in range(len(eng_text_split))])\n",
    "    pol_text_punc_removed = ' '.join([re.sub('[^A-Za-z]+', '', pol_text_split[i]) for i in range(len(pol_text_split))])\n",
    "#     pol_text_punc_removed = ' '.join([re.sub('[^A-Za-zćąęńóśźżł]+', '', pol_text_split[i]) for i in range(len(pol_text_split))])\n",
    "    \n",
    "    \n",
    "    eng_text_nums_removed = ' '.join([word.lower() for word in eng_text_punc_removed.split() if word.isalpha()])\n",
    "    pol_text_nums_removed = ' '.join([word.lower() for word in pol_text_punc_removed.split() if word.isalpha()])\n",
    "    \n",
    "    clean_text_translated.append([eng_text_nums_removed,pol_text_nums_removed])\n",
    "clean_text_translated     = np.array(clean_text_translated)\n",
    "print('Punctuations removed and all the sentences converted to smaller case.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clean text translations are below(contains first 10 translations):-\n",
      "  [['hi' 'cze']\n",
      " ['run' 'uciekaj']\n",
      " ['run' 'biegnij']\n",
      " ['run' 'uciekaj']\n",
      " ['who' 'kto']\n",
      " ['wow' 'o dziamdzia zaprzaa jej szad']\n",
      " ['wow' 'a']\n",
      " ['help' 'pomocy']\n",
      " ['jump' 'skok']]\n",
      "\n",
      "\n",
      "The clean text translations for last 3 translations are shown below :- \n",
      " [['since there are usually multiple websites on any given topic i usually just click the back button when i arrive on any webpage that has popup advertising i just go to the next page found by google and hope for something less irritating'\n",
      "  'zwykle jest wiele stron internetowych na kady temat wic kiedy trafiam na stron z popupami najczciej wciskam guzik wstecz i id do nastpnej strony znalezionej przez googlea by trafi na co mniej denerwujcego']\n",
      " ['if you want to sound like a native speaker you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo'\n",
      "  'jeli chcesz mwi jak rodzimy uytkownik musisz powtarza to samo zdanie raz za razem tak jak grajcy na banjo wicz t sam melodi a bd w stanie zagra j w podanym tempie']\n",
      " ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker'\n",
      "  'jeli kto kto nas nie zna mwi e mwimy jego jzykiem jak rodzimy uytkownik oznacza to e pewnie zauway u nas co co uwiadomio mu e tym uytkownikiem nie jestemy innymi sowy e nie mwimy jak rodzimy uytkownik']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The clean text translations are below(contains first 10 translations):-\\n ',clean_text_translated[1:10])\n",
    "print('\\n')\n",
    "print('The clean text translations for last 3 translations are shown below :- \\n',clean_text_translated[-3:])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:- Train test split \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Split :</b> We use 80% data for training and remaining 20% data for testing. However, we will be working with only a part of the dataset (first 4000 translations) because of memory constraints of my system\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_translated = clean_text_translated[:4000]\n",
    "shuffle(clean_text_translated)\n",
    "splitting_boundary   = int(0.8*len(clean_text_translated))\n",
    "train, test          = clean_text_translated[:splitting_boundary], clean_text_translated[splitting_boundary+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 :- Tokenizing and Encoding \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tokenizers:</b> We tokenize the English and Polish vocabulary separately because they are having different structures.  <br> </br>\n",
    "    <b> Encoders: </b> We encode the text of both Polish and English language based on the tokenizers declared. <br> </br>\n",
    "    <b> One hot encode : </b> We will one hot encode the target data <br><br>\n",
    "    Since we are doing Polish to English translation, we will have the english translation as the target data and we will be encoding the target data with respect to the English Vocabulary.\n",
    "    \n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct words in the English text is  1141\n",
      "The maximum length of sentence in English is         5\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer       = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(clean_text_translated[:,0])\n",
    "eng_vocab           = len(eng_tokenizer.word_index) + 1\n",
    "eng_length          = max([len(clean_text_translated[:,0][i].split()) for i in range(len(clean_text_translated[:,0]))])\n",
    "print('The number of distinct words in the English text is ',eng_vocab)\n",
    "print('The maximum length of sentence in English is        ',eng_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct words in the Polish text is  1671\n",
      "The maximum length of sentence in Polish is         8\n"
     ]
    }
   ],
   "source": [
    "pol_tokenizer       = Tokenizer()\n",
    "pol_tokenizer.fit_on_texts(clean_text_translated[:,1])\n",
    "pol_vocab           = len(pol_tokenizer.word_index) + 1\n",
    "pol_length          = max([len(clean_text_translated[:,1][i].split()) for i in range(len(clean_text_translated[:,1]))])\n",
    "print('The number of distinct words in the Polish text is ',pol_vocab)\n",
    "print('The maximum length of sentence in Polish is        ',pol_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "def encode_output(sequences, vocab_size):\n",
    "    y_list = []\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        y_list.append(encoded)\n",
    "    y = np.array(y_list)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_X = encode_sequences(pol_tokenizer, pol_length, train[:, 1])\n",
    "train_Y = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "train_Y = encode_output(train_Y, eng_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_X = encode_sequences(pol_tokenizer, pol_length, test[:, 1])\n",
    "test_Y = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "test_Y = encode_output(test_Y, eng_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 :- Building the model\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Bidirectional Encoder:</b> We use a bidirectional Encoder LSTM applied on the Polish dataset <br>\n",
    "    <b> Repeatvectors: </b> We carry forward the hidden layer output of the the bidirectional Encoder as an input to each hidden layer of the bidirectional Decoder. <br>\n",
    "    <b> Bidirectional Decoder: </b> We will use a bidirectional Decoder LSTM to get the hidden layer output distributed to the maximum vocab size of English language. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Batfleck\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 8, 20)             33420     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 20)             80        \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1200)              2985600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1200)              4800      \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 1200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 5, 1200)           8649600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 5, 1200)           4800      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 1141)           1370341   \n",
      "=================================================================\n",
      "Total params: 13,048,641\n",
      "Trainable params: 13,043,801\n",
      "Non-trainable params: 4,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model  = Sequential()\n",
    "model.add(Embedding(pol_vocab, 20, input_length=pol_length))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(CuDNNLSTM(600)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(RepeatVector(eng_length))\n",
    "model.add(Bidirectional(CuDNNLSTM(600, return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Dense(eng_vocab, activation='softmax')))\n",
    "model.compile(optimizer=adam(lr=0.005), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 :- Creating Checkpoints and getting outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr  = ReduceLROnPlateau(monitor='val_acc', factor=0.02,verbose=1,\n",
    "                              patience=5, min_lr=0.0001)\n",
    "es         = EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto', baseline=None, \n",
    "                          restore_best_weights=True)\n",
    "filepath   = os.getcwd()+'\\\\chkpts\\\\'+\"weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Batfleck\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3200 samples, validate on 799 samples\n",
      "Epoch 1/120\n",
      "3200/3200 [==============================] - 8s 2ms/step - loss: 4.5351 - acc: 0.4505 - val_loss: 9.0746 - val_acc: 2.5031e-04\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00025, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-01-4.54.hdf5\n",
      "Epoch 2/120\n",
      "3200/3200 [==============================] - 2s 569us/step - loss: 1.6507 - acc: 0.7026 - val_loss: 13.4905 - val_acc: 0.0118\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.00025 to 0.01176, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-02-1.65.hdf5\n",
      "Epoch 3/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.8993 - acc: 0.7996 - val_loss: 12.3209 - val_acc: 0.0193\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.01176 to 0.01927, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-03-0.90.hdf5\n",
      "Epoch 4/120\n",
      "3200/3200 [==============================] - 2s 575us/step - loss: 0.6003 - acc: 0.8531 - val_loss: 12.3386 - val_acc: 0.0200\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.01927 to 0.02003, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-04-0.60.hdf5\n",
      "Epoch 5/120\n",
      "3200/3200 [==============================] - 2s 569us/step - loss: 0.4521 - acc: 0.8895 - val_loss: 8.8636 - val_acc: 0.1001\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.02003 to 0.10013, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-05-0.45.hdf5\n",
      "Epoch 6/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.4040 - acc: 0.9056 - val_loss: 3.8957 - val_acc: 0.4711\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.10013 to 0.47109, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-06-0.40.hdf5\n",
      "Epoch 7/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.3373 - acc: 0.9238 - val_loss: 3.4303 - val_acc: 0.5549\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.47109 to 0.55494, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-07-0.34.hdf5\n",
      "Epoch 8/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.2785 - acc: 0.9378 - val_loss: 3.8751 - val_acc: 0.4959\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.55494\n",
      "Epoch 9/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.2633 - acc: 0.9443 - val_loss: 3.3879 - val_acc: 0.5597\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.55494 to 0.55970, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-09-0.26.hdf5\n",
      "Epoch 10/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.2423 - acc: 0.9504 - val_loss: 3.0242 - val_acc: 0.6233\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.55970 to 0.62328, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-10-0.24.hdf5\n",
      "Epoch 11/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.2089 - acc: 0.9568 - val_loss: 3.5566 - val_acc: 0.6005\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62328\n",
      "Epoch 12/120\n",
      "3200/3200 [==============================] - 2s 572us/step - loss: 0.2004 - acc: 0.9595 - val_loss: 4.3342 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62328\n",
      "Epoch 13/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.1913 - acc: 0.9596 - val_loss: 4.4946 - val_acc: 0.5582\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62328\n",
      "Epoch 14/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.1732 - acc: 0.9627 - val_loss: 3.6034 - val_acc: 0.6330\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.62328 to 0.63304, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-14-0.17.hdf5\n",
      "Epoch 15/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.1469 - acc: 0.9680 - val_loss: 3.0391 - val_acc: 0.6866\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.63304 to 0.68661, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-15-0.15.hdf5\n",
      "Epoch 16/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.1357 - acc: 0.9704 - val_loss: 2.8039 - val_acc: 0.7021\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.68661 to 0.70213, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-16-0.14.hdf5\n",
      "Epoch 17/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.1253 - acc: 0.9719 - val_loss: 2.9612 - val_acc: 0.6999\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.70213\n",
      "Epoch 18/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.1247 - acc: 0.9713 - val_loss: 2.7288 - val_acc: 0.7164\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.70213 to 0.71640, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-18-0.12.hdf5\n",
      "Epoch 19/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.1155 - acc: 0.9742 - val_loss: 2.7515 - val_acc: 0.7176\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.71640 to 0.71765, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-19-0.12.hdf5\n",
      "Epoch 20/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.1042 - acc: 0.9752 - val_loss: 2.6982 - val_acc: 0.7237\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.71765 to 0.72365, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-20-0.10.hdf5\n",
      "Epoch 21/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.0997 - acc: 0.9731 - val_loss: 2.6632 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.72365 to 0.72441, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-21-0.10.hdf5\n",
      "Epoch 22/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.0915 - acc: 0.9758 - val_loss: 2.6622 - val_acc: 0.7242\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.72441\n",
      "Epoch 23/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0881 - acc: 0.9744 - val_loss: 2.5738 - val_acc: 0.7294\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.72441 to 0.72941, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-23-0.09.hdf5\n",
      "Epoch 24/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0960 - acc: 0.9753 - val_loss: 2.5356 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.72941 to 0.73016, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\Machine translation Seq2Seq\\chkpts\\weights-improvement-24-0.10.hdf5\n",
      "Epoch 25/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.0869 - acc: 0.9754 - val_loss: 2.5267 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.73016\n",
      "Epoch 26/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0858 - acc: 0.9756 - val_loss: 2.6558 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.73016\n",
      "Epoch 27/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0820 - acc: 0.9765 - val_loss: 2.5883 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.73016\n",
      "Epoch 28/120\n",
      "3200/3200 [==============================] - 2s 571us/step - loss: 0.0815 - acc: 0.9746 - val_loss: 2.6351 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.73016\n",
      "Epoch 29/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0870 - acc: 0.9746 - val_loss: 2.5812 - val_acc: 0.7242\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.73016\n",
      "Epoch 30/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0705 - acc: 0.9797 - val_loss: 2.5775 - val_acc: 0.7244\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.73016\n",
      "Epoch 31/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0680 - acc: 0.9808 - val_loss: 2.5757 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.73016\n",
      "Epoch 32/120\n",
      "3200/3200 [==============================] - 2s 569us/step - loss: 0.0660 - acc: 0.9806 - val_loss: 2.5741 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.73016\n",
      "Epoch 33/120\n",
      "3200/3200 [==============================] - 2s 569us/step - loss: 0.0644 - acc: 0.9808 - val_loss: 2.5741 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.73016\n",
      "Epoch 34/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0633 - acc: 0.9810 - val_loss: 2.5745 - val_acc: 0.7259\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.73016\n",
      "Epoch 35/120\n",
      "3200/3200 [==============================] - 2s 569us/step - loss: 0.0618 - acc: 0.9811 - val_loss: 2.5753 - val_acc: 0.7262\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.73016\n",
      "Epoch 36/120\n",
      "3200/3200 [==============================] - 2s 570us/step - loss: 0.0612 - acc: 0.9811 - val_loss: 2.5758 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.73016\n",
      "Epoch 37/120\n",
      "3200/3200 [==============================] - 2s 569us/step - loss: 0.0601 - acc: 0.9815 - val_loss: 2.5770 - val_acc: 0.7269\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.73016\n",
      "Epoch 38/120\n",
      "3200/3200 [==============================] - 2s 569us/step - loss: 0.0595 - acc: 0.9813 - val_loss: 2.5784 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.73016\n",
      "Epoch 39/120\n",
      "3200/3200 [==============================] - 2s 569us/step - loss: 0.0594 - acc: 0.9815 - val_loss: 2.5788 - val_acc: 0.7267\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.73016\n",
      "Epoch 00039: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x218820cf048>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=120, batch_size=128, validation_data=(test_X, test_Y), \n",
    "          callbacks        = [es,reduce_lr,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 :- Getting Predictions \n",
    "We finally use the model to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_int(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, value):\n",
    "    prediction = model.predict(value, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = []\n",
    "    for i in integers:\n",
    "        word = word_int(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47, 31, 37,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47, 31, 37,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[798]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source    = []\n",
    "target    = []\n",
    "predicted = []\n",
    "for i in range(30):\n",
    "        value = test_X[i]\n",
    "        value = value.reshape((1, value.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, value)\n",
    "        val1, val2 = test[i]\n",
    "        target.append(val1)\n",
    "        source.append(val2)\n",
    "        predicted.append(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the translation results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrcie</td>\n",
       "      <td>youre back</td>\n",
       "      <td>youre back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nienawidz toma</td>\n",
       "      <td>i hate tom</td>\n",
       "      <td>i hate tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chce mi si spa</td>\n",
       "      <td>i want to sleep</td>\n",
       "      <td>i feel face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to mj dom</td>\n",
       "      <td>this is my home</td>\n",
       "      <td>thats my my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dzwonie</td>\n",
       "      <td>did you call</td>\n",
       "      <td>did you call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to musi by tutaj</td>\n",
       "      <td>it must be here</td>\n",
       "      <td>it is tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed za wami tskni</td>\n",
       "      <td>i will miss you</td>\n",
       "      <td>i came for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>poka mi</td>\n",
       "      <td>show me</td>\n",
       "      <td>show me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nie zamiecaj</td>\n",
       "      <td>dont litter</td>\n",
       "      <td>dont litter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>zapaciem rachunki</td>\n",
       "      <td>i paid my bills</td>\n",
       "      <td>i paid bills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>to jest nielegalne</td>\n",
       "      <td>thats illegal</td>\n",
       "      <td>thats is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mam dwa samochody</td>\n",
       "      <td>i have two cars</td>\n",
       "      <td>i have a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>popatrz tutaj</td>\n",
       "      <td>look here</td>\n",
       "      <td>look here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to jest przeraajce</td>\n",
       "      <td>thats creepy</td>\n",
       "      <td>thats is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>przesta kama</td>\n",
       "      <td>stop lying</td>\n",
       "      <td>stop lying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>we mnie do toma</td>\n",
       "      <td>take me to tom</td>\n",
       "      <td>now work me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tom jest nieudacznikiem</td>\n",
       "      <td>tom is a loser</td>\n",
       "      <td>tom is a loser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mielimy racj</td>\n",
       "      <td>we were right</td>\n",
       "      <td>we were right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>musiaam wyj</td>\n",
       "      <td>i had to leave</td>\n",
       "      <td>i an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>to pilne</td>\n",
       "      <td>its urgent</td>\n",
       "      <td>its urgent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bd powany</td>\n",
       "      <td>get serious</td>\n",
       "      <td>be patient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>prosz bardzo nie ma za co</td>\n",
       "      <td>youre welcome</td>\n",
       "      <td>please cant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>czy zraniem ci</td>\n",
       "      <td>did i hurt you</td>\n",
       "      <td>did i hurt you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>skocz ni</td>\n",
       "      <td>stop dreaming</td>\n",
       "      <td>tom a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>to jest hazard</td>\n",
       "      <td>its a gamble</td>\n",
       "      <td>its is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>widzielimy go</td>\n",
       "      <td>weve seen him</td>\n",
       "      <td>i kissed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>znam go</td>\n",
       "      <td>i know him</td>\n",
       "      <td>i know him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nie zabijaj mnie</td>\n",
       "      <td>dont kill me</td>\n",
       "      <td>dont kill me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tom przyjecha</td>\n",
       "      <td>tom drove</td>\n",
       "      <td>tom drove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tom wanie przyszed</td>\n",
       "      <td>tom just came</td>\n",
       "      <td>tom just came</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Source           Target       Predicted\n",
       "0                       wrcie       youre back      youre back\n",
       "1              nienawidz toma       i hate tom      i hate tom\n",
       "2              chce mi si spa  i want to sleep     i feel face\n",
       "3                   to mj dom  this is my home     thats my my\n",
       "4                     dzwonie     did you call    did you call\n",
       "5            to musi by tutaj  it must be here       it is tom\n",
       "6           bed za wami tskni  i will miss you      i came for\n",
       "7                     poka mi          show me         show me\n",
       "8                nie zamiecaj      dont litter     dont litter\n",
       "9           zapaciem rachunki  i paid my bills    i paid bills\n",
       "10         to jest nielegalne    thats illegal        thats is\n",
       "11          mam dwa samochody  i have two cars        i have a\n",
       "12              popatrz tutaj        look here       look here\n",
       "13         to jest przeraajce     thats creepy        thats is\n",
       "14               przesta kama       stop lying      stop lying\n",
       "15            we mnie do toma   take me to tom     now work me\n",
       "16    tom jest nieudacznikiem   tom is a loser  tom is a loser\n",
       "17               mielimy racj    we were right   we were right\n",
       "18                musiaam wyj   i had to leave            i an\n",
       "19                   to pilne       its urgent      its urgent\n",
       "20                  bd powany      get serious      be patient\n",
       "21  prosz bardzo nie ma za co    youre welcome     please cant\n",
       "22             czy zraniem ci   did i hurt you  did i hurt you\n",
       "23                   skocz ni    stop dreaming           tom a\n",
       "24             to jest hazard     its a gamble          its is\n",
       "25              widzielimy go    weve seen him        i kissed\n",
       "26                    znam go       i know him      i know him\n",
       "27           nie zabijaj mnie     dont kill me    dont kill me\n",
       "28              tom przyjecha        tom drove       tom drove\n",
       "29         tom wanie przyszed    tom just came   tom just came"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf = pd.DataFrame(columns=['Source','Target','Predicted'])\n",
    "finaldf['Source'] = source\n",
    "finaldf['Target'] = target\n",
    "finaldf['Predicted'] = predicted\n",
    "finaldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comments:</b>  <br>\n",
    "    <b> Accuracy:-</b> We were able to get fairly decent translations done from Polish to English. But as the words increase the accuracy tends to be on the lesser side. We don't really get exact match to the target phrases. <br>\n",
    "    <b> Alternatives :- </b> We can use attention mechanism to get better results. The Seq2Seq architecture works fairly well on small sequences of data but for bigger sequences, we need to extract the words from the phrases that are more important than others. <br>\n",
    "    <b> Dataset size :- </b> Due to extensive computational infrastructure needed for Seq2Seq models, I have limited the data size. The dataset has less number of bigger sentences as compared to smaller ones. Hence it has got much less data to train on bigger sequences of words. We can train the model on the entire dataset to get better results (if infrastructure permits)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
