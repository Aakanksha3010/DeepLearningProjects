{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import time \n",
    "import random\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Loading data for creating the model:</b> We will load the dataset containing question and answer pairs for creating the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_professional = pd.read_csv('qna_chitchat_caring.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Data Preprocessing:</b> The preprocessing step involves picking up the columnsrelated to questions and answers only, splitting the texts to remove punctuations and numbers and finally adding 'start of sentence' and 'end of sentence' token that will help the model in knowing where a word is starting and where it is ending. The process will be followed both for questions and answers data separately \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations removed and all the sentences converted to smaller case.\n",
      "Example of preprocessed Question Sentence is :-  <start> youre in a good mood today <end>\n",
      "Example of preprocessed Answer Sentence is :-  <start> im quite content <end>\n"
     ]
    }
   ],
   "source": [
    "clean_text                  = []\n",
    "for i in range(len(df_professional)):\n",
    "    ques_text               = df_professional.loc[i]['Question']\n",
    "    ans_text                = df_professional.loc[i]['Answer']\n",
    "    \n",
    "    ques_text_split         = ques_text.split()\n",
    "    ans_text_split          = ans_text.split()\n",
    "    \n",
    "    ques_text__punc_removed = ' '.join([re.sub('[^A-Za-z]+', '', ques_text_split[i]) for i in range(len(ques_text_split))])\n",
    "    ans_text_punc_removed   = ' '.join([re.sub('[^A-Za-z]+', '', ans_text_split[i]) for i in range(len(ans_text_split))])\n",
    "    \n",
    "    \n",
    "    ques_text_nums_removed  = ' '.join([word.lower() for word in ques_text__punc_removed.split() if word.isalpha()])\n",
    "    ans_text_nums_removed   = ' '.join([word.lower() for word in ans_text_punc_removed.split() if word.isalpha()])\n",
    "    \n",
    "    ques_text_final         = '<start> '+ques_text_nums_removed + ' <end>'\n",
    "    ans_text_final          = '<start> '+ans_text_nums_removed + ' <end>'\n",
    "    clean_text.append([ques_text_final,ans_text_final])\n",
    "clean_text                  = np.array(clean_text)\n",
    "print('Punctuations removed and all the sentences converted to smaller case.')\n",
    "print('Example of preprocessed Question Sentence is :- ',clean_text[1200][0])\n",
    "print('Example of preprocessed Answer Sentence is :- ',clean_text[1200][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tokenizing:</b> Both Questions and Answers data will be tokenized to get the distinct words and maximum length of a sentence in a Question set as well as in an answer set (Tx and Ty). We will use the tokenizers later in the training process. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct words in the Questions is  2030\n",
      "The maximum length of sentence in Question is     17\n"
     ]
    }
   ],
   "source": [
    "ques_tokenizer       = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "ques_tokenizer.fit_on_texts(clean_text[:,0])\n",
    "question_vocab       = len(ques_tokenizer.word_index) + 1\n",
    "question_length      = max([len(clean_text[:,0][i].split()) for i in range(len(clean_text[:,0]))])\n",
    "print('The number of distinct words in the Questions is ',question_vocab)\n",
    "print('The maximum length of sentence in Question is    ',question_length)\n",
    "ques_tensor_tokenized= ques_tokenizer.texts_to_sequences(clean_text[:,0])\n",
    "tensor_question      = tf.keras.preprocessing.sequence.pad_sequences(ques_tensor_tokenized,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct words in the Answers is  237\n",
      "The maximum length of sentence in Answers is    21\n"
     ]
    }
   ],
   "source": [
    "ans_tokenizer        = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "ans_tokenizer.fit_on_texts(clean_text[:,1])\n",
    "ans_vocab            = len(ans_tokenizer.word_index) + 1\n",
    "ans_length           = max([len(clean_text[:,1][i].split()) for i in range(len(clean_text[:,1]))])\n",
    "print('The number of distinct words in the Answers is ',ans_vocab)\n",
    "print('The maximum length of sentence in Answers is   ',ans_length)\n",
    "ans_tensor_tokenized = ans_tokenizer.texts_to_sequences(clean_text[:,1])\n",
    "tensor_answer        = tf.keras.preprocessing.sequence.pad_sequences(ans_tensor_tokenized,padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Train Test Split :</b> The data will be divided to train and test split.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(tensor_question, tensor_answer, test_size=0.2)\n",
    "max_length_targ = input_tensor_val.shape[1]\n",
    "max_length_inp  = input_tensor_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Encoder class:</b> The encoder class takes the questions and input and outputs a thought vector in form of the hidden state of a GRU. We also initialize the hidden state to zeros to ease the training process (we could have initialized it randomly as well)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Attention Layer:</b> Since we have obtained the thoughtvector from the Encoder layer, we will use attention weights to transform the thought vector to a context vector where the weights corresponding to respective input places (from 1 to Tx) is applied to the thought vector in form of a function. The context vector will be the input to the Decoder layer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score                 = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights     = tf.nn.softmax(score, axis=1)\n",
    "        context_vector        = attention_weights * values\n",
    "        context_vector        = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Decoder Layer:</b> This module will take the encoder hidden state and then convert it to context vector which will be used as an input to the GRU state. The output will be a vector of size (less than) or equal to Ty. That output hidden layer will be preprocessed to get the anwer to the question that was input in the Encoder layer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz  = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru       = tf.keras.layers.GRU(self.dec_units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        self.fc        = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x                                 = self.embedding(x)\n",
    "        x                                 = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state                     = self.gru(x)\n",
    "        output                            = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x                                 = self.fc(output)\n",
    "        return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Parameters to the model:</b> Some parameters are set for the model. However we will use the parameter tuning for embedding dimensions, units(hiddden units for Attention layer)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE     = len(input_tensor_train)\n",
    "BATCH_SIZE      = 32\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim   = 256\n",
    "units           = 1048\n",
    "vocab_inp_size  = len(ques_tokenizer.word_index)+1\n",
    "vocab_tar_size  = len(ans_tokenizer.word_index)+1\n",
    "dataset         = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset         = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Loss function :</b> Loss function will be created to be used to measure losses at teacher forcing step\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask   = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_  = loss_object(real, pred)\n",
    "    mask   = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Training step :</b> Here we are going to train the model and enforce teacher forcing steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        enc_output, enc_hidden     = encoder(inp, enc_hidden)\n",
    "    \n",
    "        dec_hidden                 = enc_hidden\n",
    "        dec_input                  = tf.expand_dims([ans_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "    \n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "    \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input                  = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss                     = (loss / int(targ.shape[1]))\n",
    "    variables                      = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients                      = tape.gradient(loss, variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hyper parameter margin and initialization:</b> We set up initial values of the Hyper parameters to start with and also specify the range within which we want to find the values of the hyper parameters. The optimal weights corresponding to encoders and decoders will be stored for the least loss attained \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate      = Real(low=1e-6, high=1e-2, prior='log-uniform',name='learning_rate')\n",
    "units              = Integer(low=600, high=1050, name='units')\n",
    "embedding_dim      = Integer(low=10,high=300,name='embedding_dim')\n",
    "#batch_size         = Integer(low=4,high=128,name='batch_size')\n",
    "dimensions         = [learning_rate,units,embedding_dim]\n",
    "default_parameters = [0.005, 650,256]\n",
    "best_accuracy      = 0.0\n",
    "path_best_model    = 'chatbot_best_model_attention.keras'\n",
    "best_loss          = 1000\n",
    "test_results       = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, units,embedding_dim):\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('Units :', units)\n",
    "    print('Embedding size:', embedding_dim)\n",
    "    EPOCHS      = 5\n",
    "    global optimizer\n",
    "    optimizer   = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    global encoder\n",
    "    encoder     = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "    global decoder\n",
    "    decoder     = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "            if batch % 50 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n",
    "        \n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss / steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    Loss = float(total_loss/ steps_per_epoch)\n",
    "    print(\"Loss: \",Loss)\n",
    "    \n",
    "    global best_loss\n",
    "    if Loss < best_loss:\n",
    "        best_loss = Loss\n",
    "        encoder_weights_path = 'D:\\\\training_checkpoints\\\\encoder_weights\\\\enc.h5'\n",
    "        decoder_weights_path = 'D:\\\\training_checkpoints\\\\decoder_weights\\\\dec.h5'\n",
    "        encoder.save_weights(encoder_weights_path)\n",
    "        decoder.save_weights(decoder_weights_path)\n",
    "        output_dict = {'encoder':encoder,'decoder':decoder,\n",
    "                       'loss':Loss,'units':int(units),'embedding_size':int(embedding_dim)}\n",
    "        test_results.append(output_dict)\n",
    "    tf.keras.backend.clear_session()\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 5.0e-03\n",
      "Units : 650\n",
      "Embedding size: 256\n",
      "Epoch 1 Batch 0 Loss 1.9285\n",
      "Epoch 1 Batch 50 Loss 0.4107\n",
      "Epoch 1 Batch 100 Loss 0.3379\n",
      "Epoch 1 Batch 150 Loss 0.2320\n",
      "Epoch 1 Loss 0.5541\n",
      "Time taken for 1 epoch 53.475921630859375 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2458\n",
      "Epoch 2 Batch 50 Loss 0.0814\n",
      "Epoch 2 Batch 100 Loss 0.0833\n",
      "Epoch 2 Batch 150 Loss 0.0491\n",
      "Epoch 2 Loss 0.1132\n",
      "Time taken for 1 epoch 53.13115859031677 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0768\n",
      "Epoch 3 Batch 50 Loss 0.0265\n",
      "Epoch 3 Batch 100 Loss 0.0238\n",
      "Epoch 3 Batch 150 Loss 0.0854\n",
      "Epoch 3 Loss 0.0504\n",
      "Time taken for 1 epoch 53.10770535469055 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0124\n",
      "Epoch 4 Batch 50 Loss 0.0225\n",
      "Epoch 4 Batch 100 Loss 0.0220\n",
      "Epoch 4 Batch 150 Loss 0.0263\n",
      "Epoch 4 Loss 0.0232\n",
      "Time taken for 1 epoch 53.08648204803467 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0202\n",
      "Epoch 5 Batch 50 Loss 0.0453\n",
      "Epoch 5 Batch 100 Loss 0.0834\n",
      "Epoch 5 Batch 150 Loss 0.0717\n",
      "Epoch 5 Loss 0.0441\n",
      "Time taken for 1 epoch 52.97817087173462 sec\n",
      "\n",
      "Loss:  0.04405184090137482\n",
      "learning rate: 1.3e-05\n",
      "Units : 620\n",
      "Embedding size: 223\n",
      "Epoch 1 Batch 0 Loss 1.8546\n",
      "Epoch 1 Batch 50 Loss 2.0018\n",
      "Epoch 1 Batch 100 Loss 1.8353\n",
      "Epoch 1 Batch 150 Loss 1.3351\n",
      "Epoch 1 Loss 1.9149\n",
      "Time taken for 1 epoch 54.575684785842896 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8068\n",
      "Epoch 2 Batch 50 Loss 1.5456\n",
      "Epoch 2 Batch 100 Loss 1.6159\n",
      "Epoch 2 Batch 150 Loss 2.0322\n",
      "Epoch 2 Loss 1.6650\n",
      "Time taken for 1 epoch 54.709354639053345 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.5429\n",
      "Epoch 3 Batch 50 Loss 1.5599\n",
      "Epoch 3 Batch 100 Loss 1.3490\n",
      "Epoch 3 Batch 150 Loss 1.5275\n",
      "Epoch 3 Loss 1.6294\n",
      "Time taken for 1 epoch 54.71779131889343 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.4558\n",
      "Epoch 4 Batch 50 Loss 1.7978\n",
      "Epoch 4 Batch 100 Loss 1.4728\n",
      "Epoch 4 Batch 150 Loss 1.4279\n",
      "Epoch 4 Loss 1.6148\n",
      "Time taken for 1 epoch 54.60632276535034 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.6321\n",
      "Epoch 5 Batch 50 Loss 1.7085\n",
      "Epoch 5 Batch 100 Loss 2.0299\n",
      "Epoch 5 Batch 150 Loss 1.6474\n",
      "Epoch 5 Loss 1.5992\n",
      "Time taken for 1 epoch 54.71127223968506 sec\n",
      "\n",
      "Loss:  1.5992201566696167\n",
      "learning rate: 1.3e-05\n",
      "Units : 988\n",
      "Embedding size: 120\n",
      "Epoch 1 Batch 0 Loss 2.1480\n",
      "Epoch 1 Batch 50 Loss 1.8966\n",
      "Epoch 1 Batch 100 Loss 1.9564\n",
      "Epoch 1 Batch 150 Loss 1.7471\n",
      "Epoch 1 Loss 1.8777\n",
      "Time taken for 1 epoch 61.40711498260498 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8473\n",
      "Epoch 2 Batch 50 Loss 1.7543\n",
      "Epoch 2 Batch 100 Loss 1.8311\n",
      "Epoch 2 Batch 150 Loss 1.7346\n",
      "Epoch 2 Loss 1.6487\n",
      "Time taken for 1 epoch 61.23540425300598 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.5784\n",
      "Epoch 3 Batch 50 Loss 1.2854\n",
      "Epoch 3 Batch 100 Loss 1.7743\n",
      "Epoch 3 Batch 150 Loss 1.6952\n",
      "Epoch 3 Loss 1.6287\n",
      "Time taken for 1 epoch 61.52477025985718 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.4231\n",
      "Epoch 4 Batch 50 Loss 1.4986\n",
      "Epoch 4 Batch 100 Loss 1.3116\n",
      "Epoch 4 Batch 150 Loss 1.6917\n",
      "Epoch 4 Loss 1.6190\n",
      "Time taken for 1 epoch 61.22686767578125 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2686\n",
      "Epoch 5 Batch 50 Loss 1.3318\n",
      "Epoch 5 Batch 100 Loss 1.4204\n",
      "Epoch 5 Batch 150 Loss 1.3759\n",
      "Epoch 5 Loss 1.6027\n",
      "Time taken for 1 epoch 61.3746337890625 sec\n",
      "\n",
      "Loss:  1.602723479270935\n",
      "learning rate: 1.7e-03\n",
      "Units : 1047\n",
      "Embedding size: 163\n",
      "Epoch 1 Batch 0 Loss 1.7089\n",
      "Epoch 1 Batch 50 Loss 1.1059\n",
      "Epoch 1 Batch 100 Loss 0.5937\n",
      "Epoch 1 Batch 150 Loss 0.3774\n",
      "Epoch 1 Loss 0.8652\n",
      "Time taken for 1 epoch 65.7226972579956 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.4490\n",
      "Epoch 2 Batch 50 Loss 0.2261\n",
      "Epoch 2 Batch 100 Loss 0.1646\n",
      "Epoch 2 Batch 150 Loss 0.1793\n",
      "Epoch 2 Loss 0.2326\n",
      "Time taken for 1 epoch 65.78603434562683 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1520\n",
      "Epoch 3 Batch 50 Loss 0.1149\n",
      "Epoch 3 Batch 100 Loss 0.1126\n",
      "Epoch 3 Batch 150 Loss 0.1003\n",
      "Epoch 3 Loss 0.1089\n",
      "Time taken for 1 epoch 65.70294308662415 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0439\n",
      "Epoch 4 Batch 50 Loss 0.0607\n",
      "Epoch 4 Batch 100 Loss 0.0802\n",
      "Epoch 4 Batch 150 Loss 0.0695\n",
      "Epoch 4 Loss 0.0580\n",
      "Time taken for 1 epoch 65.7521584033966 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0248\n",
      "Epoch 5 Batch 50 Loss 0.0324\n",
      "Epoch 5 Batch 100 Loss 0.0289\n",
      "Epoch 5 Batch 150 Loss 0.0332\n",
      "Epoch 5 Loss 0.0345\n",
      "Time taken for 1 epoch 65.728590965271 sec\n",
      "\n",
      "Loss:  0.03453652560710907\n",
      "learning rate: 2.2e-06\n",
      "Units : 674\n",
      "Embedding size: 256\n",
      "Epoch 1 Batch 0 Loss 1.9689\n",
      "Epoch 1 Batch 50 Loss 2.1382\n",
      "Epoch 1 Batch 100 Loss 2.4212\n",
      "Epoch 1 Batch 150 Loss 2.1281\n",
      "Epoch 1 Loss 1.9719\n",
      "Time taken for 1 epoch 55.76835012435913 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8993\n",
      "Epoch 2 Batch 50 Loss 1.8413\n",
      "Epoch 2 Batch 100 Loss 1.9851\n",
      "Epoch 2 Batch 150 Loss 1.7565\n",
      "Epoch 2 Loss 1.9661\n",
      "Time taken for 1 epoch 55.399022340774536 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.8519\n",
      "Epoch 3 Batch 50 Loss 2.0357\n",
      "Epoch 3 Batch 100 Loss 1.8016\n",
      "Epoch 3 Batch 150 Loss 2.0739\n",
      "Epoch 3 Loss 1.9532\n",
      "Time taken for 1 epoch 55.20396685600281 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5934\n",
      "Epoch 4 Batch 50 Loss 1.9778\n",
      "Epoch 4 Batch 100 Loss 1.7192\n",
      "Epoch 4 Batch 150 Loss 1.6046\n",
      "Epoch 4 Loss 1.8705\n",
      "Time taken for 1 epoch 55.300108909606934 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.6468\n",
      "Epoch 5 Batch 50 Loss 2.0548\n",
      "Epoch 5 Batch 100 Loss 1.6454\n",
      "Epoch 5 Batch 150 Loss 1.7359\n",
      "Epoch 5 Loss 1.7289\n",
      "Time taken for 1 epoch 55.46960377693176 sec\n",
      "\n",
      "Loss:  1.728924036026001\n",
      "learning rate: 2.8e-05\n",
      "Units : 699\n",
      "Embedding size: 46\n",
      "Epoch 1 Batch 0 Loss 2.0507\n",
      "Epoch 1 Batch 50 Loss 1.9065\n",
      "Epoch 1 Batch 100 Loss 1.4403\n",
      "Epoch 1 Batch 150 Loss 1.5515\n",
      "Epoch 1 Loss 1.8171\n",
      "Time taken for 1 epoch 53.53808951377869 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7404\n",
      "Epoch 2 Batch 50 Loss 1.4930\n",
      "Epoch 2 Batch 100 Loss 2.0388\n",
      "Epoch 2 Batch 150 Loss 1.5734\n",
      "Epoch 2 Loss 1.6390\n",
      "Time taken for 1 epoch 53.519731760025024 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.9181\n",
      "Epoch 3 Batch 50 Loss 1.5578\n",
      "Epoch 3 Batch 100 Loss 1.9167\n",
      "Epoch 3 Batch 150 Loss 1.7319\n",
      "Epoch 3 Loss 1.6266\n",
      "Time taken for 1 epoch 53.46632266044617 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.6103\n",
      "Epoch 4 Batch 50 Loss 1.6671\n",
      "Epoch 4 Batch 100 Loss 1.4555\n",
      "Epoch 4 Batch 150 Loss 1.4571\n",
      "Epoch 4 Loss 1.6083\n",
      "Time taken for 1 epoch 53.57833194732666 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4259\n",
      "Epoch 5 Batch 50 Loss 1.8292\n",
      "Epoch 5 Batch 100 Loss 1.5059\n",
      "Epoch 5 Batch 150 Loss 1.6829\n",
      "Epoch 5 Loss 1.5643\n",
      "Time taken for 1 epoch 53.35550236701965 sec\n",
      "\n",
      "Loss:  1.5642831325531006\n",
      "learning rate: 3.2e-03\n",
      "Units : 629\n",
      "Embedding size: 82\n",
      "Epoch 1 Batch 0 Loss 1.7006\n",
      "Epoch 1 Batch 50 Loss 1.0636\n",
      "Epoch 1 Batch 100 Loss 0.5067\n",
      "Epoch 1 Batch 150 Loss 0.3579\n",
      "Epoch 1 Loss 0.8304\n",
      "Time taken for 1 epoch 53.48060989379883 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2852\n",
      "Epoch 2 Batch 50 Loss 0.2155\n",
      "Epoch 2 Batch 100 Loss 0.1986\n",
      "Epoch 2 Batch 150 Loss 0.1499\n",
      "Epoch 2 Loss 0.2145\n",
      "Time taken for 1 epoch 53.73264932632446 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1255\n",
      "Epoch 3 Batch 50 Loss 0.1215\n",
      "Epoch 3 Batch 100 Loss 0.0764\n",
      "Epoch 3 Batch 150 Loss 0.0749\n",
      "Epoch 3 Loss 0.1000\n",
      "Time taken for 1 epoch 53.71809267997742 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0580\n",
      "Epoch 4 Batch 50 Loss 0.0432\n",
      "Epoch 4 Batch 100 Loss 0.0647\n",
      "Epoch 4 Batch 150 Loss 0.0316\n",
      "Epoch 4 Loss 0.0522\n",
      "Time taken for 1 epoch 53.73254346847534 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0251\n",
      "Epoch 5 Batch 50 Loss 0.0352\n",
      "Epoch 5 Batch 100 Loss 0.0301\n",
      "Epoch 5 Batch 150 Loss 0.0201\n",
      "Epoch 5 Loss 0.0310\n",
      "Time taken for 1 epoch 53.75850772857666 sec\n",
      "\n",
      "Loss:  0.031044503673911095\n",
      "learning rate: 4.5e-03\n",
      "Units : 923\n",
      "Embedding size: 239\n",
      "Epoch 1 Batch 0 Loss 1.8225\n",
      "Epoch 1 Batch 50 Loss 0.4105\n",
      "Epoch 1 Batch 100 Loss 0.3379\n",
      "Epoch 1 Batch 150 Loss 0.3541\n",
      "Epoch 1 Loss 0.6650\n",
      "Time taken for 1 epoch 62.401936769485474 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.4114\n",
      "Epoch 2 Batch 50 Loss 0.3687\n",
      "Epoch 2 Batch 100 Loss 0.3467\n",
      "Epoch 2 Batch 150 Loss 0.3168\n",
      "Epoch 2 Loss 0.3136\n",
      "Time taken for 1 epoch 62.23312997817993 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2329\n",
      "Epoch 3 Batch 50 Loss 0.1860\n",
      "Epoch 3 Batch 100 Loss 0.1974\n",
      "Epoch 3 Batch 150 Loss 0.2469\n",
      "Epoch 3 Loss 0.2208\n",
      "Time taken for 1 epoch 62.37135457992554 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1643\n",
      "Epoch 4 Batch 50 Loss 0.2426\n",
      "Epoch 4 Batch 100 Loss 0.1932\n",
      "Epoch 4 Batch 150 Loss 0.1501\n",
      "Epoch 4 Loss 0.1911\n",
      "Time taken for 1 epoch 62.216472864151 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1716\n",
      "Epoch 5 Batch 50 Loss 0.1584\n",
      "Epoch 5 Batch 100 Loss 0.1565\n",
      "Epoch 5 Batch 150 Loss 0.1225\n",
      "Epoch 5 Loss 0.1630\n",
      "Time taken for 1 epoch 62.164711236953735 sec\n",
      "\n",
      "Loss:  0.16295182704925537\n",
      "learning rate: 1.6e-03\n",
      "Units : 627\n",
      "Embedding size: 70\n",
      "Epoch 1 Batch 0 Loss 2.2374\n",
      "Epoch 1 Batch 50 Loss 1.4339\n",
      "Epoch 1 Batch 100 Loss 0.7717\n",
      "Epoch 1 Batch 150 Loss 0.5666\n",
      "Epoch 1 Loss 1.0900\n",
      "Time taken for 1 epoch 53.02965974807739 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 0.4611\n",
      "Epoch 2 Batch 50 Loss 0.3484\n",
      "Epoch 2 Batch 100 Loss 0.3527\n",
      "Epoch 2 Batch 150 Loss 0.3108\n",
      "Epoch 2 Loss 0.3873\n",
      "Time taken for 1 epoch 53.71919560432434 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2621\n",
      "Epoch 3 Batch 50 Loss 0.2993\n",
      "Epoch 3 Batch 100 Loss 0.2560\n",
      "Epoch 3 Batch 150 Loss 0.1988\n",
      "Epoch 3 Loss 0.2207\n",
      "Time taken for 1 epoch 53.84934759140015 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1871\n",
      "Epoch 4 Batch 50 Loss 0.1707\n",
      "Epoch 4 Batch 100 Loss 0.1572\n",
      "Epoch 4 Batch 150 Loss 0.1082\n",
      "Epoch 4 Loss 0.1328\n",
      "Time taken for 1 epoch 54.034343957901 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1076\n",
      "Epoch 5 Batch 50 Loss 0.0893\n",
      "Epoch 5 Batch 100 Loss 0.0667\n",
      "Epoch 5 Batch 150 Loss 0.0841\n",
      "Epoch 5 Loss 0.0810\n",
      "Time taken for 1 epoch 53.58395004272461 sec\n",
      "\n",
      "Loss:  0.08102484792470932\n",
      "learning rate: 8.4e-05\n",
      "Units : 863\n",
      "Embedding size: 179\n",
      "Epoch 1 Batch 0 Loss 1.9854\n",
      "Epoch 1 Batch 50 Loss 1.4924\n",
      "Epoch 1 Batch 100 Loss 1.5235\n",
      "Epoch 1 Batch 150 Loss 1.7869\n",
      "Epoch 1 Loss 1.6852\n",
      "Time taken for 1 epoch 59.26762270927429 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7966\n",
      "Epoch 2 Batch 50 Loss 1.5350\n",
      "Epoch 2 Batch 100 Loss 1.4638\n",
      "Epoch 2 Batch 150 Loss 1.6727\n",
      "Epoch 2 Loss 1.5108\n",
      "Time taken for 1 epoch 59.13165497779846 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.6308\n",
      "Epoch 3 Batch 50 Loss 1.1856\n",
      "Epoch 3 Batch 100 Loss 1.1332\n",
      "Epoch 3 Batch 150 Loss 1.3151\n",
      "Epoch 3 Loss 1.2819\n",
      "Time taken for 1 epoch 59.23690390586853 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.2077\n",
      "Epoch 4 Batch 50 Loss 1.1019\n",
      "Epoch 4 Batch 100 Loss 1.0927\n",
      "Epoch 4 Batch 150 Loss 0.8678\n",
      "Epoch 4 Loss 1.0479\n",
      "Time taken for 1 epoch 59.22775340080261 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.8410\n",
      "Epoch 5 Batch 50 Loss 1.1908\n",
      "Epoch 5 Batch 100 Loss 0.9002\n",
      "Epoch 5 Batch 150 Loss 0.6529\n",
      "Epoch 5 Loss 0.8576\n",
      "Time taken for 1 epoch 59.127575635910034 sec\n",
      "\n",
      "Loss:  0.8575936555862427\n",
      "learning rate: 2.8e-04\n",
      "Units : 901\n",
      "Embedding size: 191\n",
      "Epoch 1 Batch 0 Loss 1.7574\n",
      "Epoch 1 Batch 50 Loss 1.7853\n",
      "Epoch 1 Batch 100 Loss 1.4564\n",
      "Epoch 1 Batch 150 Loss 1.4193\n",
      "Epoch 1 Loss 1.5094\n",
      "Time taken for 1 epoch 61.10417652130127 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0434\n",
      "Epoch 2 Batch 50 Loss 0.9339\n",
      "Epoch 2 Batch 100 Loss 0.7352\n",
      "Epoch 2 Batch 150 Loss 0.5411\n",
      "Epoch 2 Loss 0.8491\n",
      "Time taken for 1 epoch 60.96358394622803 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.7689\n",
      "Epoch 3 Batch 50 Loss 0.4180\n",
      "Epoch 3 Batch 100 Loss 0.4611\n",
      "Epoch 3 Batch 150 Loss 0.4740\n",
      "Epoch 3 Loss 0.4824\n",
      "Time taken for 1 epoch 65.32566380500793 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3856\n",
      "Epoch 4 Batch 50 Loss 0.3212\n",
      "Epoch 4 Batch 100 Loss 0.2547\n",
      "Epoch 4 Batch 150 Loss 0.3520\n",
      "Epoch 4 Loss 0.3133\n",
      "Time taken for 1 epoch 65.04907155036926 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.2338\n",
      "Epoch 5 Batch 50 Loss 0.2493\n",
      "Epoch 5 Batch 100 Loss 0.2155\n",
      "Epoch 5 Batch 150 Loss 0.2573\n",
      "Epoch 5 Loss 0.2346\n",
      "Time taken for 1 epoch 65.48776769638062 sec\n",
      "\n",
      "Loss:  0.23464250564575195\n",
      "learning rate: 1.0e-02\n",
      "Units : 600\n",
      "Embedding size: 10\n",
      "Epoch 1 Batch 0 Loss 2.1481\n",
      "Epoch 1 Batch 50 Loss 1.6176\n",
      "Epoch 1 Batch 100 Loss 1.3528\n",
      "Epoch 1 Batch 150 Loss 0.5723\n",
      "Epoch 1 Loss 1.3058\n",
      "Time taken for 1 epoch 56.08202886581421 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.5852\n",
      "Epoch 2 Batch 50 Loss 0.4013\n",
      "Epoch 2 Batch 100 Loss 0.3768\n",
      "Epoch 2 Batch 150 Loss 0.3528\n",
      "Epoch 2 Loss 0.4352\n",
      "Time taken for 1 epoch 55.598376750946045 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.3370\n",
      "Epoch 3 Batch 50 Loss 0.3458\n",
      "Epoch 3 Batch 100 Loss 0.2911\n",
      "Epoch 3 Batch 150 Loss 0.2757\n",
      "Epoch 3 Loss 0.2907\n",
      "Time taken for 1 epoch 56.139204263687134 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.2611\n",
      "Epoch 4 Batch 50 Loss 0.2997\n",
      "Epoch 4 Batch 100 Loss 0.3152\n",
      "Epoch 4 Batch 150 Loss 0.2637\n",
      "Epoch 4 Loss 0.2481\n",
      "Time taken for 1 epoch 55.37082648277283 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.2368\n",
      "Epoch 5 Batch 50 Loss 0.2172\n",
      "Epoch 5 Batch 100 Loss 0.2249\n",
      "Epoch 5 Batch 150 Loss 0.2373\n",
      "Epoch 5 Loss 0.2178\n",
      "Time taken for 1 epoch 55.27617788314819 sec\n",
      "\n",
      "Loss:  0.21775281429290771\n",
      "learning rate: 8.7e-04\n",
      "Units : 600\n",
      "Embedding size: 300\n",
      "Epoch 1 Batch 0 Loss 1.7903\n",
      "Epoch 1 Batch 50 Loss 1.2260\n",
      "Epoch 1 Batch 100 Loss 0.8967\n",
      "Epoch 1 Batch 150 Loss 0.7045\n",
      "Epoch 1 Loss 1.0716\n",
      "Time taken for 1 epoch 58.4499990940094 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.4485\n",
      "Epoch 2 Batch 50 Loss 0.4091\n",
      "Epoch 2 Batch 100 Loss 0.3311\n",
      "Epoch 2 Batch 150 Loss 0.2949\n",
      "Epoch 2 Loss 0.3944\n",
      "Time taken for 1 epoch 58.49470257759094 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2331\n",
      "Epoch 3 Batch 50 Loss 0.2937\n",
      "Epoch 3 Batch 100 Loss 0.2031\n",
      "Epoch 3 Batch 150 Loss 0.1592\n",
      "Epoch 3 Loss 0.2227\n",
      "Time taken for 1 epoch 58.319122076034546 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1476\n",
      "Epoch 4 Batch 50 Loss 0.1270\n",
      "Epoch 4 Batch 100 Loss 0.1094\n",
      "Epoch 4 Batch 150 Loss 0.0890\n",
      "Epoch 4 Loss 0.1318\n",
      "Time taken for 1 epoch 58.51798105239868 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0909\n",
      "Epoch 5 Batch 50 Loss 0.0749\n",
      "Epoch 5 Batch 100 Loss 0.0766\n",
      "Epoch 5 Batch 150 Loss 0.0724\n",
      "Epoch 5 Loss 0.0770\n",
      "Time taken for 1 epoch 58.45140027999878 sec\n",
      "\n",
      "Loss:  0.07701055705547333\n",
      "learning rate: 2.0e-03\n",
      "Units : 600\n",
      "Embedding size: 300\n",
      "Epoch 1 Batch 0 Loss 2.2135\n",
      "Epoch 1 Batch 50 Loss 0.8350\n",
      "Epoch 1 Batch 100 Loss 0.4829\n",
      "Epoch 1 Batch 150 Loss 0.3685\n",
      "Epoch 1 Loss 0.7632\n",
      "Time taken for 1 epoch 58.79093360900879 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2253\n",
      "Epoch 2 Batch 50 Loss 0.1949\n",
      "Epoch 2 Batch 100 Loss 0.2063\n",
      "Epoch 2 Batch 150 Loss 0.1902\n",
      "Epoch 2 Loss 0.2274\n",
      "Time taken for 1 epoch 59.176931619644165 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1569\n",
      "Epoch 3 Batch 50 Loss 0.1346\n",
      "Epoch 3 Batch 100 Loss 0.1481\n",
      "Epoch 3 Batch 150 Loss 0.0943\n",
      "Epoch 3 Loss 0.1082\n",
      "Time taken for 1 epoch 58.491199254989624 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0727\n",
      "Epoch 4 Batch 50 Loss 0.0700\n",
      "Epoch 4 Batch 100 Loss 0.0292\n",
      "Epoch 4 Batch 150 Loss 0.0287\n",
      "Epoch 4 Loss 0.0550\n",
      "Time taken for 1 epoch 58.50323724746704 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0208\n",
      "Epoch 5 Batch 50 Loss 0.0215\n",
      "Epoch 5 Batch 100 Loss 0.0248\n",
      "Epoch 5 Batch 150 Loss 0.0295\n",
      "Epoch 5 Loss 0.0272\n",
      "Time taken for 1 epoch 58.4945752620697 sec\n",
      "\n",
      "Loss:  0.027185842394828796\n",
      "learning rate: 2.2e-03\n",
      "Units : 1050\n",
      "Embedding size: 300\n",
      "Epoch 1 Batch 0 Loss 2.2866\n",
      "Epoch 1 Batch 50 Loss 0.5693\n",
      "Epoch 1 Batch 100 Loss 0.4851\n",
      "Epoch 1 Batch 150 Loss 0.2871\n",
      "Epoch 1 Loss 0.6692\n",
      "Time taken for 1 epoch 71.36506009101868 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2637\n",
      "Epoch 2 Batch 50 Loss 0.1782\n",
      "Epoch 2 Batch 100 Loss 0.1921\n",
      "Epoch 2 Batch 150 Loss 0.1027\n",
      "Epoch 2 Loss 0.1613\n",
      "Time taken for 1 epoch 68.08545112609863 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0891\n",
      "Epoch 3 Batch 50 Loss 0.0807\n",
      "Epoch 3 Batch 100 Loss 0.0805\n",
      "Epoch 3 Batch 150 Loss 0.0394\n",
      "Epoch 3 Loss 0.0659\n",
      "Time taken for 1 epoch 67.58146500587463 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0502\n",
      "Epoch 4 Batch 50 Loss 0.0306\n",
      "Epoch 4 Batch 100 Loss 0.0300\n",
      "Epoch 4 Batch 150 Loss 0.0336\n",
      "Epoch 4 Loss 0.0335\n",
      "Time taken for 1 epoch 67.5903992652893 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0387\n",
      "Epoch 5 Batch 50 Loss 0.0276\n",
      "Epoch 5 Batch 100 Loss 0.0200\n",
      "Epoch 5 Batch 150 Loss 0.0197\n",
      "Epoch 5 Loss 0.0190\n",
      "Time taken for 1 epoch 67.4609727859497 sec\n",
      "\n",
      "Loss:  0.018955636769533157\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=15,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hyper parameter tuning resuts: </b> We can see that the tuning of the parameters gave us results and out minimum loss was attained at the 15th call to the training function by the minimizer. We could have got better results if we increased the number of calls to the training function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Minimum Loss reached is :-  0.018955636769533157\n"
     ]
    }
   ],
   "source": [
    "print('The Minimum Loss reached is :- ',search_result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "units         = search_result.x[1]\n",
    "embedding_dim = search_result.x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22bb9d40748>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Evaluate: </b> We will now create a function to apply the model prepared with the tuned parameters to any input question sentence to provide the desired output in form of an answer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    optimizer   = tf.keras.optimizers.Adam(learning_rate=search_result.x[0])\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    input_ques_split        = sentence.split()\n",
    "    ques_text_punc_removed = ' '.join([re.sub('[^A-Za-z]+', '', input_ques_split[i]) for i in range(len(input_ques_split))])\n",
    "    ques_text_nums_removed = ' '.join([word.lower() for word in ques_text_punc_removed.split() if word.isalpha()])\n",
    "    sentence                  = '<start> '+ques_text_nums_removed + ' <end>'\n",
    "    inputs = [ques_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                       maxlen=input_tensor_train.shape[1],\n",
    "                                                       padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    encoder.load_weights('D:\\\\training_checkpoints\\\\encoder_weights\\\\enc.h5')\n",
    "    decoder.load_weights('D:\\\\training_checkpoints\\\\decoder_weights\\\\dec.h5')\n",
    "    \n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([ans_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(input_tensor_val.shape[1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += ans_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if ans_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Generate Conversations: </b> The following code will generate the conversations randomly. We will also check the answer given by out bot and what was actually expected from it\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversation_randomly():\n",
    "    word_index_dictionary_inputs = ques_tokenizer.word_index\n",
    "    question_index_word_dict     =  {v: k for k, v in word_index_dictionary_inputs.items()}\n",
    "\n",
    "    ans_ques_index_dictionary_inputs = ans_tokenizer.word_index\n",
    "    answer_index_word_dict           =  {v: k for k, v in ans_ques_index_dictionary_inputs.items()}\n",
    "\n",
    "    j             = random.randint(0,1200)\n",
    "    question_list = [question_index_word_dict[i] if i>0 else '' for i in input_tensor_val[j] ] \n",
    "    input_ques    = ' '.join(question_list)\n",
    "    input_questn  = input_ques.replace('<end>','').replace('<start>','')\n",
    "\n",
    "\n",
    "    ans_sentence_list = [answer_index_word_dict[i] if i>0 else '' for i in target_tensor_val[j]] \n",
    "    input_answer      = ' '.join(ans_sentence_list)\n",
    "    input_answer_exp  = input_answer.replace('<end>','').replace('<start>','')\n",
    "    print('Query           :-',input_questn.strip())\n",
    "    print('Bat-Bot reply   :-',evaluate(input_questn.strip()).replace('<end>',\"\"))\n",
    "    print('Answer Expected :-',input_answer_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query           :- between you and me who is more cute\n",
      "Bat-Bot reply   :- i truly have no way to tell  \n",
      "Answer Expected :-  i truly have no way to tell             \n",
      "\n",
      "\n",
      "Query           :- this again\n",
      "Bat-Bot reply   :- good night  \n",
      "Answer Expected :-  i do repeat myself sometimes its something im working on          \n",
      "\n",
      "\n",
      "Query           :- dont worry bot ill be back\n",
      "Bat-Bot reply   :- take your time ill be here  \n",
      "Answer Expected :-  take your time ill be here              \n",
      "\n",
      "\n",
      "Query           :- should i go to therapy\n",
      "Bat-Bot reply   :- i trust your judgment  \n",
      "Answer Expected :-  i trust your judgment                \n",
      "\n",
      "\n",
      "Query           :- is anyone available to chat to me\n",
      "Bat-Bot reply   :- sure ask me a question  \n",
      "Answer Expected :-  sure ask me a question               \n",
      "\n",
      "\n",
      "Query           :- should i go to therapy\n",
      "Bat-Bot reply   :- i trust your judgment  \n",
      "Answer Expected :-  i trust your judgment                \n",
      "\n",
      "\n",
      "Query           :- im canadian\n",
      "Bat-Bot reply   :- okay  \n",
      "Answer Expected :-  okay                   \n",
      "\n",
      "\n",
      "Query           :- i really like you\n",
      "Bat-Bot reply   :- thank you that made my day  \n",
      "Answer Expected :-  thank you that made my day              \n",
      "\n",
      "\n",
      "Query           :- i will return shortly\n",
      "Bat-Bot reply   :- take your time ill be here  \n",
      "Answer Expected :-  take your time ill be here              \n",
      "\n",
      "\n",
      "Query           :- go away\n",
      "Bat-Bot reply   :- if you need anything later just let me know  \n",
      "Answer Expected :-  if you need anything later just let me know           \n",
      "\n",
      "\n",
      "Query           :- which one of us is younger\n",
      "Bat-Bot reply   :- i dont have an age  \n",
      "Answer Expected :-  i dont have an age               \n",
      "\n",
      "\n",
      "Query           :- youve said that before\n",
      "Bat-Bot reply   :- my apologies  \n",
      "Answer Expected :-  i do repeat myself sometimes its something im working on          \n",
      "\n",
      "\n",
      "Query           :- how was this lovely day for you then\n",
      "Bat-Bot reply   :- im great thanks for asking  \n",
      "Answer Expected :-  lovely thanks for asking                \n",
      "\n",
      "\n",
      "Query           :- im on clould nine\n",
      "Bat-Bot reply   :- okay  \n",
      "Answer Expected :-  thats simply wonderful                 \n",
      "\n",
      "\n",
      "Query           :- youre not entertaining\n",
      "Bat-Bot reply   :- oh dear  \n",
      "Answer Expected :-  oh dear                  \n",
      "\n",
      "\n",
      "Query           :- you are crazy boring\n",
      "Bat-Bot reply   :- oh dear  \n",
      "Answer Expected :-  oh dear                  \n",
      "\n",
      "\n",
      "Query           :- say a funny saying\n",
      "Bat-Bot reply   :- cattywampus i always get a good giggle out of weird words  \n",
      "Answer Expected :-  cattywampus i always get a good giggle out of weird words         \n",
      "\n",
      "\n",
      "Query           :- loool\n",
      "Bat-Bot reply   :- excellent  \n",
      "Answer Expected :-  youve got me laughing too               \n",
      "\n",
      "\n",
      "Query           :- you complete me\n",
      "Bat-Bot reply   :- im not sure  \n",
      "Answer Expected :-  thats so kind of you               \n",
      "\n",
      "\n",
      "Query           :- im having a bad day\n",
      "Bat-Bot reply   :- thats simply wonderful  \n",
      "Answer Expected :-  im so sorry                 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    generate_conversation_randomly()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion: </b> The answers are better than Seq2Seq models and we are getting complete answers to the questions asked. However there are some answers that are not as expected. We can improve the efficiency of our bot by increasing the sample size or allowing more callbacks to the tuning function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
