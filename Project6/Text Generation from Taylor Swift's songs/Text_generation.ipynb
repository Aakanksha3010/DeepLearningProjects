{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 :- Importing necessary libraries for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, CuDNNLSTM, Dropout, Bidirectional\n",
    "from keras.callbacks import *\n",
    "from keras.utils import to_categorical\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 :- Loading input dataset \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Data Source :</b> The data is downloaded from - https://github.com/irenetrampoline/taylor-swift-lyrics  <br><br>\n",
    " We load the dataset which is in .txt format and display the first 100 characters of text. (The text will contain special characters like new lines, tabs, brackets etc. We will be doing the preprocessing in the next step. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 characters is given by:- \n",
      "He said the way my blue eyes shined\n",
      "Put those Georgia stars to shame that night\n",
      "I said, \"That's a li\n"
     ]
    }
   ],
   "source": [
    "path_to_file = r'D:\\kaggle_trials\\taylorswiftsongs'+'\\\\all_tswift_lyrics.txt'\n",
    "file = open(path_to_file,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "print('The first 100 characters is given by:- ')\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 :- Preprocessing the data\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Preprocessing involved:</b> The only preprocessing we are going to do is to convert the text to lowercase. We will be using the first 100,000 characters of the dataset in order to save time and get results quickly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 lines of pre-processed data will be :- \n",
      "  He said the way my blue eyes shined\n",
      "Put those Georgia stars to shame that night\n",
      "I said, \"That's a li\n"
     ]
    }
   ],
   "source": [
    "text_to_use = text.lower()\n",
    "text_to_use = text[:100000]\n",
    "print('The first 100 lines of pre-processed data will be :- \\n ', text_to_use[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 :- Feature Extraction steps \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Step 1: </b> Extracting all characters of the text to be used for analysis. We also determine the unique set of characters to be used later for indexing.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of characters used in the songs is given by :  100000\n",
      "The unique character size is  74\n"
     ]
    }
   ],
   "source": [
    "n_chars      = len(text_to_use)\n",
    "unique_vocab = list(set(text_to_use))\n",
    "print('The number of characters used in the songs is given by : ', n_chars)\n",
    "print('The unique character size is ',len(unique_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us look at the unique characters in the songs:- \n",
      " ['z', '(', ' ', '}', 'B', 'G', 'd', 'E', 'Y', '\"', 'm', 'k', 'p', 'T', \"'\", ':', 'R', 'b', 'n', 'w', '2', '5', ']', ')', 's', 'O', 'f', 'l', 'I', 'v', 'u', 'F', '[', ';', '4', 'S', '.', 'U', 'J', 'q', 'j', 'e', 'H', 'W', 'a', '\\n', ',', 'A', 'y', 'P', '9', '?', 'C', 'L', 'M', '8', '3', 'x', 'Q', 'K', 'D', 'V', 'N', '-', 'r', '1', 'i', 'o', '{', 'h', 't', 'g', 'c', '!']\n"
     ]
    }
   ],
   "source": [
    "print('Let us look at the unique characters in the songs:- \\n',unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = {n:char for n, char in enumerate(unique_vocab)}\n",
    "char_to_int = {char:n for n, char in enumerate(unique_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 :- Data preparation \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Step 1 (Preparing X tensor): </b> We will create the X and y tensors to be used in the model. The X tensor will be of shape (sample size,sequence_length,features). In our case, the sample size is 100000, the sequence length is 100 and features is 1 (i.e, we are using 100 previous characters to predict the 101th character. <br>\n",
    "    <b> Step 1.1 Normalize the vectors:- </b>We will further normalize each elements of the tensor to get a value between zero and one(much like min-max scalar) by dividing each integer represntation of token by number of distinct characters. <br>\n",
    "    <b> Step 2 (Preparing Y tensor): </b> Here the y tensor Will be the actual 101th value(character) to be treated as a target. <br>\n",
    "    <b> Step 3 (Reshaping the X tensor): </b> The shape of X tensor will be of form (sample size,sequence_length,features)<br>\n",
    "    <b> Step 4 (One hot encoding Y tensor): </b> The y tensor will be one hot encoded to be passed as an input to the LSTm model we will be using.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new shape: (99900, 100, 1)\n",
      "y_new shape: (99900, 74)\n"
     ]
    }
   ],
   "source": [
    "X           = []\n",
    "y           = []\n",
    "seq_length  = 100\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in  = text_to_use[i:i + seq_length]\n",
    "    seq_out = text_to_use[i + seq_length]\n",
    "    X.append([char_to_int[char] for char in seq_in])\n",
    "    y.append(char_to_int[seq_out])\n",
    "X_new       = np.reshape(X, (len(X), seq_length, 1)) \n",
    "X_new       = X_new/(float(len(unique_vocab)))\n",
    "y_new       = to_categorical(y) \n",
    "print(\"X_new shape:\", X_new.shape)\n",
    "print(\"y_new shape:\", y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 :- Creating LSTM model for text generation\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Bidirectional LSTM + Dense layers:</b> We fit a bidirectional LSTM (because we are working with data having no trend values). We will use the last layer's hidden units to a fully connected network to predict the next word. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 100, 400)          324800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 400)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 200)               401600    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 74)                14874     \n",
      "=================================================================\n",
      "Total params: 741,274\n",
      "Trainable params: 741,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(200, return_sequences=True), input_shape=(X_new.shape[1], X_new.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(CuDNNLSTM(100)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_new.shape[1], activation='softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Callback Creation\n",
    "We create callbacks which we will be using while execution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,verbose=1,\n",
    "                              patience=5, min_lr=0.0001)\n",
    "es        = EarlyStopping(monitor='loss', patience=5, verbose=1, mode='auto', baseline=None, \n",
    "                          restore_best_weights=True)\n",
    "filepath   = os.getcwd()+'\\\\chkpts\\\\'+\"weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "99900/99900 [==============================] - 69s 689us/step - loss: 3.0518\n",
      "Epoch 2/60\n",
      "99900/99900 [==============================] - 68s 681us/step - loss: 2.7891\n",
      "Epoch 3/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 2.6367\n",
      "Epoch 4/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 2.5793\n",
      "Epoch 5/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 2.4771\n",
      "Epoch 6/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 2.3824\n",
      "Epoch 7/60\n",
      "99900/99900 [==============================] - 69s 690us/step - loss: 2.2859\n",
      "Epoch 8/60\n",
      "99900/99900 [==============================] - 69s 691us/step - loss: 2.2027\n",
      "Epoch 9/60\n",
      "99900/99900 [==============================] - 69s 691us/step - loss: 2.1390\n",
      "Epoch 10/60\n",
      "99900/99900 [==============================] - 69s 688us/step - loss: 2.0699\n",
      "Epoch 11/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 2.0057\n",
      "Epoch 12/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.9552\n",
      "Epoch 13/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.9004\n",
      "Epoch 14/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.8684\n",
      "Epoch 15/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.8286\n",
      "Epoch 16/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.7672\n",
      "Epoch 17/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.7391\n",
      "Epoch 18/60\n",
      "99900/99900 [==============================] - 69s 689us/step - loss: 1.6948\n",
      "Epoch 19/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.66721s - \n",
      "Epoch 20/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.6357\n",
      "Epoch 21/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.6009\n",
      "Epoch 22/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.5744\n",
      "Epoch 23/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.5418\n",
      "Epoch 24/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.5186\n",
      "Epoch 25/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.5057\n",
      "Epoch 26/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.4793\n",
      "Epoch 27/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.4643\n",
      "Epoch 28/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.4345\n",
      "Epoch 29/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.4158\n",
      "Epoch 30/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.3967\n",
      "Epoch 31/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.3878\n",
      "Epoch 32/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.3629\n",
      "Epoch 33/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.3513\n",
      "Epoch 34/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.3377\n",
      "Epoch 35/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.3225\n",
      "Epoch 36/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.3028\n",
      "Epoch 37/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.2983\n",
      "Epoch 38/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.2900\n",
      "Epoch 39/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.2707\n",
      "Epoch 40/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.2586\n",
      "Epoch 41/60\n",
      "99900/99900 [==============================] - 68s 684us/step - loss: 1.2642\n",
      "Epoch 42/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.2350\n",
      "Epoch 43/60\n",
      "99900/99900 [==============================] - 69s 686us/step - loss: 1.2326\n",
      "Epoch 44/60\n",
      "99900/99900 [==============================] - 69s 688us/step - loss: 1.2210\n",
      "Epoch 45/60\n",
      "99900/99900 [==============================] - 69s 686us/step - loss: 1.2010\n",
      "Epoch 46/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.1925\n",
      "Epoch 47/60\n",
      "99900/99900 [==============================] - 68s 685us/step - loss: 1.2069\n",
      "Epoch 48/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.1703\n",
      "Epoch 49/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.1668\n",
      "Epoch 50/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.1566\n",
      "Epoch 51/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.1452\n",
      "Epoch 52/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.4066\n",
      "Epoch 53/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.3988\n",
      "Epoch 54/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.2752\n",
      "Epoch 55/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.2631\n",
      "Epoch 56/60\n",
      "99900/99900 [==============================] - 68s 683us/step - loss: 1.8012\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29833fae400>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 60\n",
    "batch_sz = 64\n",
    "model.fit(X_new, y_new, \n",
    "          epochs = epochs, \n",
    "          batch_size = batch_sz,\n",
    "         callbacks        = [reduce_lr,es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 :- Generating text from the model developed\n",
    "We generate 60 further characters from a given line of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o low\n",
      "You can't feel nothing at all\n",
      "And you flashback to\n",
      "When he said forever and always\n",
      "And it rain\n"
     ]
    }
   ],
   "source": [
    "token_string = X[np.random.randint(0, len(X)-1)]\n",
    "complete_string = [int_to_char[value] for value in token_string]\n",
    "print (''.join(complete_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_string = []\n",
    "for i in range(60):\n",
    "    x = np.reshape(token_string, (1, len(token_string), 1))\n",
    "    x = x / float(len(unique_vocab))\n",
    "    \n",
    "    prediction = model.predict(x, verbose=0)\n",
    "\n",
    "    id_pred = np.argmax(prediction)\n",
    "    seq_in = [int_to_char[value] for value in token_string]\n",
    "    \n",
    "    generate_string.append(int_to_char[id_pred])\n",
    "    \n",
    "    token_string.append(id_pred)\n",
    "    token_string = token_string[1:len(token_string)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String of characters provided :- \n",
      " o low\n",
      "You can't feel nothing at all\n",
      "And you flashback to\n",
      "When he said forever and always\n",
      "And it rain\n",
      "\n",
      "\n",
      "The text completed is:- \n",
      "  o low\n",
      "You can't feel nothing at all\n",
      "And you flashback to\n",
      "When he said forever and always\n",
      "And it rains in your bedroom\n",
      "Everything is wrong\n",
      "It rains when you're g\n"
     ]
    }
   ],
   "source": [
    "print('String of characters provided :- \\n',''.join(complete_string))\n",
    "print('\\n')\n",
    "text = \"\"\n",
    "for char in complete_string+generate_string:\n",
    "    text = text + char\n",
    "print('The text completed is:- \\n ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
