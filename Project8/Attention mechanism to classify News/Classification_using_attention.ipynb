{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 :- Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Model, Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, CuDNNLSTM, Dropout, TimeDistributed, Reshape, Activation, Dot, RepeatVector\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks import *\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "from IPython.core.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 :- Reading the dataset and performing elementary preprocessing on texts.\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Elementary preprocessing done :</b> The preprocessing task involves removal of punctuation and converting all uppercase characters to lower. <br><br>\n",
    "    <b> The data is loaded from https://www.kaggle.com/c/learn-ai-bbc/data</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politics</td>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sport</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sport</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "5       politics  howard hits back at mongrel jibe michael howar...\n",
       "6       politics  blair prepares to name poll date tony blair is...\n",
       "7          sport  henman hopes ended in dubai third seed tim hen...\n",
       "8          sport  wilkinson fit to face edinburgh england captai...\n",
       "9  entertainment  last star wars  not for children  the sixth an..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'bbc.csv')\n",
    "df['text'] = df['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "df['text'] = df['text'].str.lower()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Text Preprocessing :-\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>text_tokens :</b> A list containing a list which in turn contains all possible vocabularies of a particular text row.<br>\n",
    "    <b>text_news :</b> A list having all possible news of column 'text' of dataframe. It is a list having all text values. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = [text.split() for text in df[\"text\"].values.tolist()]\n",
    "text_news  = df[\"text\"].values.tolist()\n",
    "labels      = df[\"category\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer     = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_news)\n",
    "word2id       = tokenizer.word_index\n",
    "id2word       = dict([(value, key) for (key, value) in word2id.items()])\n",
    "vocab_size    = len(word2id) + 1\n",
    "embedding_dim = 150\n",
    "max_len       = 200\n",
    "X             = [[word2id[word] for word in sent] for sent in text_tokens]\n",
    "X_pad         = pad_sequences(X, maxlen=max_len, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (shape): (2225, 200)\n",
      "y (shape): (2225, 5)\n"
     ]
    }
   ],
   "source": [
    "label2id      = {l: i for i, l in enumerate(set(labels))}\n",
    "id2label      = {v: k for k, v in label2id.items()}\n",
    "y             = [label2id[label] for label in labels]\n",
    "y             = to_categorical(y, num_classes=len(label2id), dtype='float32')\n",
    "print(\"X (shape): {}\".format(X_pad.shape))\n",
    "print(\"y (shape): {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Model Building\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Encoder-Decoder with Attention:</b> We will first build a simple LSTM model and get its hidden layers to form a context vector to be input into the second decoder LSTM and finally use dense layers to predict the class\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 150)     4965750     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 200, 150)     0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 200, 300)     362400      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 200, 300)     0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 200, 1)       301         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 200)          0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Activation)      (None, 200)          0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 300)          0           dropout_6[0][0]                  \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_3 (RepeatVector)  (None, 200, 300)     0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 300)          542400      repeat_vector_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5)            1505        bidirectional_6[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 5,872,356\n",
      "Trainable params: 5,872,356\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_input    = Input(shape=(max_len,), dtype='int32')\n",
    "embedded     = Embedding(vocab_size,\n",
    "                          embedding_dim,\n",
    "                          input_length=max_len)(seq_input)\n",
    "embedded     = Dropout(0.2)(embedded)\n",
    "lstm_encoder = Bidirectional(CuDNNLSTM(embedding_dim, return_sequences=True))(embedded)\n",
    "lstm_encoder = Dropout(0.2)(lstm_encoder)\n",
    "attn_vector  = TimeDistributed(Dense(1))(lstm_encoder)\n",
    "attn_vector  = Reshape((max_len,))(attn_vector)\n",
    "attn_vector  = Activation('softmax', name='attention_vec')(attn_vector)\n",
    "attn_output  = Dot(axes=1)([lstm_encoder, attn_vector])\n",
    "context      = RepeatVector(200)(attn_output)\n",
    "lstm_decoder = Bidirectional(CuDNNLSTM(embedding_dim,return_sequences=False))(context)\n",
    "output       = Dense(len(label2id), activation='softmax')(lstm_decoder)\n",
    "model        = Model(inputs = [seq_input],outputs = output)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Creating callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr  = ReduceLROnPlateau(monitor='val_acc', factor=0.02,verbose=1,\n",
    "                              patience=5, min_lr=0.0001)\n",
    "es         = EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto', baseline=None, \n",
    "                          restore_best_weights=True)\n",
    "filepath   = os.getcwd()+'\\\\chkpts\\\\'+\"weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Running the model to get output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/30\n",
      "1780/1780 [==============================] - 4s 3ms/step - loss: 1.4926 - acc: 0.3191 - val_loss: 1.4163 - val_acc: 0.4180\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.41798, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\News Classification using attention\\chkpts\\weights-improvement-01-1.49.hdf5\n",
      "Epoch 2/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 1.2025 - acc: 0.5056 - val_loss: 0.9497 - val_acc: 0.6270\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.41798 to 0.62697, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\News Classification using attention\\chkpts\\weights-improvement-02-1.20.hdf5\n",
      "Epoch 3/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.6054 - acc: 0.7433 - val_loss: 0.6707 - val_acc: 0.7551\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.62697 to 0.75506, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\News Classification using attention\\chkpts\\weights-improvement-03-0.61.hdf5\n",
      "Epoch 4/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.2678 - acc: 0.9230 - val_loss: 0.5648 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.75506 to 0.85843, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\News Classification using attention\\chkpts\\weights-improvement-04-0.27.hdf5\n",
      "Epoch 5/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.1100 - acc: 0.9713 - val_loss: 0.3362 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85843 to 0.89438, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\News Classification using attention\\chkpts\\weights-improvement-05-0.11.hdf5\n",
      "Epoch 6/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0265 - acc: 0.9955 - val_loss: 0.3749 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.89438 to 0.91910, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\News Classification using attention\\chkpts\\weights-improvement-06-0.03.hdf5\n",
      "Epoch 7/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.4209 - val_acc: 0.9213\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.91910 to 0.92135, saving model to C:\\Users\\Batfleck\\APB_DL_EXERCISES\\News Classification using attention\\chkpts\\weights-improvement-07-0.00.hdf5\n",
      "Epoch 8/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.5876 - val_acc: 0.8989\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92135\n",
      "Epoch 9/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.4373 - acc: 0.8730 - val_loss: 0.7462 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92135\n",
      "Epoch 10/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.1843 - acc: 0.9624 - val_loss: 0.4097 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92135\n",
      "Epoch 11/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.1086 - acc: 0.9663 - val_loss: 0.4145 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92135\n",
      "Epoch 12/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0403 - acc: 0.9916 - val_loss: 0.4574 - val_acc: 0.8697\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92135\n",
      "Epoch 13/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0229 - acc: 0.9961 - val_loss: 0.4170 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92135\n",
      "Epoch 14/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0114 - acc: 0.9983 - val_loss: 0.3986 - val_acc: 0.8876\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92135\n",
      "Epoch 15/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0077 - acc: 0.9994 - val_loss: 0.3921 - val_acc: 0.8921\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92135\n",
      "Epoch 16/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0060 - acc: 0.9994 - val_loss: 0.3939 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92135\n",
      "Epoch 17/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.4010 - val_acc: 0.8921\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92135\n",
      "Epoch 18/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.4066 - val_acc: 0.8921\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92135\n",
      "Epoch 19/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.4126 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92135\n",
      "Epoch 20/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.4201 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92135\n",
      "Epoch 21/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.4277 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.92135\n",
      "Epoch 22/30\n",
      "1780/1780 [==============================] - 3s 1ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.4341 - val_acc: 0.8944\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.92135\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2493f295e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_pad, y, epochs=30, batch_size=128, validation_split=0.2, shuffle=True,\n",
    "         callbacks        = [es,reduce_lr,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Validating the model to get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_att = Model(inputs=model.input,\n",
    "                  outputs=[model.output, model.get_layer('attention_vec').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_value        = random.randint(0,len(df))\n",
    "sample_text         = df.iloc[random_value]['text']\n",
    "associated_category = df.iloc[random_value]['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample text is: \n",
      "\n",
      " playstation 3 processor unveiled the cell processor  which will drive sony s playstation 3  will run 10times faster than current pc chips  its designers have said  sony  ibm and toshiba  who have been working on the cell processor for three years  unveiled the chip on monday it is being designed for use in graphics workstations  the new playstation console  and has been described as a supercomputer on a chip the chip will run at speeds of greater than 4 ghz  the firms said by comparison  rival chip maker intel s fastest processor runs at 38 ghz details of the chip were released at the international solid state circuits conference in san francisco the new processor is set to ignite a fresh battle between intel and the cell consortium over which processor sits at the centre of digital products the playstation 3 is expected in 2006  while toshiba plans to incorporate it into highend televisions next year ibm has said it will sell a workstation with the chip starting later this year  cell is comprised of several computing engines  or cores a core based on ibm s power architecture controls eight  synergistic  processing centres in all  they can simultaneously carry out 10 instruction sequences  compared with two for current intel chips  later this year  intel and advanced micro devices plan to release their own  multicore  chips  which also increase the number of instructions that can be executed at once the cell s specifications suggest the playstation 3 will offer a significant boost in graphics capabilities but analysts cautioned that not all the features in a product announcement will find their way into systems  any new technology like this has two components   said steve kleynhans  an analyst with meta group he said  it has the vision of what it could be because you need the big vision to sell it  then there s the reality of how it s really going to be used  which generally is several levels down the chain from there   while the playstation 3 is likely to be the first massmarket product to use cell  the chip s designers have said the flexible architecture means that it would be useful for a wide range of applications  from servers to mobile phones initial devices are unlikely to be any smaller than a games console  however  because the first version of the cell will run hot enough to need a cooling fan and while marketing speak describes the chip as a  supercomputer   it remains significantly slower than the slowest computer on the list of the world s top 500 supercomputers ibm said cell was  os neutral  and would support multiple operating systems simultaneously but designers would not confirm if microsoft s windows was among those tested with the chip if cell is to challenge intel s range of chips in the marketplace  it will need to find itself inside pcs  which predominantly run using windows\n",
      "\n",
      "\n",
      "The associated category is:  tech\n"
     ]
    }
   ],
   "source": [
    "#sample_text = random.choice(df[\"text\"].values.tolist())\n",
    "print('The sample text is: \\n\\n',sample_text)\n",
    "print('\\n')\n",
    "print('The associated category is: ',associated_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sample = sample_text.split()\n",
    "encoded_samples = [[word2id[word] for word in tokenized_sample]]\n",
    "encoded_samples = pad_sequences(encoded_samples, maxlen=max_len)\n",
    "label_probs, attentions = model_att.predict(encoded_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'politics': 9.615334e-05, 'entertainment': 0.00056732853, 'business': 0.0016372548, 'sport': 8.549465e-06, 'tech': 0.9976907}\n"
     ]
    }
   ],
   "source": [
    "label_probs = {id2label[_id]: prob for (label, _id), prob in zip(label2id.items(),label_probs[0])}\n",
    "print(label_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class will be  tech\n"
     ]
    }
   ],
   "source": [
    "print('The predicted class will be ',max(label_probs, key=label_probs.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
